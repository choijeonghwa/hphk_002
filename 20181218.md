# 2018.12.18

> ##### 복습
>
> python 문법
>
> 1. 저장
>
> 2. 조건
>
>    ```python
>    if (조건문):
>        실행문
>    elif (조건문):
>        실행문
>    else:
>        실행문
>    ```
>
> 3. 반복
>
>    ```python
>    while(True):
>        반복문
>    for i in dust:
>        반복문
>    ```



#### 로또

```python
# 1부터 45까지의 숫자를 가진 numbers라는 배열을 만든다
# python make long array / make number array

#numbers에서 숫자 6개를 랜덤으로 뽑는다.(비복원 추출)
#랜덤으로 뽑은 숫자들을 lotto 변수에 담고 출력한다.

# 추가 : lotto 변수에 담겨있는 숫자들을 오름차순으로 정렬하기

import random

i=1

numbers=[]

#while i<=45:
#  numbers.append(i)
#  i+=1

for i in range(45):
  numbers.append(i+1)

lotto=random.sample(numbers, 6)

lotto.sort()
print(lotto)
```

- 배열에 추가할 때 `배열이름.append(i)` 사용
- range() 함수의 경우

``` python
리스트 = list(range(3))
0
1
2
리스트 = list(range(2,5))
2
3
4

```

- list() 함수 안에  range() 함수 넣을 경우 list로 만들 수 있음

- `random.sample(배열, 선택할 갯수)` 로 랜덤으로 선택할 수 있음





#### 코스피

- 웹에서 오른쪽 마우스 버튼 클릭 ==> 검사 버튼



> ##### 미세먼지 코드 복습
>
> ```python
> import requests
> # 사람들이 만든 특수한 라이브러리(후에 다운받아서 사용해야함)
> from bs4 import BeautifulSoup
> 
> 
> url = 'http://openapi.airkorea.or.kr/openapi/services/rest/ArpltnInforInqireSvc/getCtprvnRltmMesureDnsty?sidoName=%EC%84%9C%EC%9A%B8&ServiceKey={}&ver=1.3&pageNo=3'.format(key)
> # URL에 요청하는 역할. 내용물 찍어오기 위해서 .text로. 그 값을 response에 담기
> response = requests.get(url).text
> # 정말 긴 문자열에서 'Ctrl+F'역할을 해주는게 BeautifulSoup. xml / html 문서에서 어느 부분을 쉽게 찾을 수 있도록 만들어줌
> soup = BeautifulSoup(response, 'xml')
> # 'item'이라는 문자열 검색해서 [7]배열에 있는 것
> gn = soup('item')[7]
> 
> location = gn.stationName.text
> time = gn.dataTime.text
> dust = int(gn.pm10Value.text)
> ```
>
> - html은 정해진 것만 사용 가능
> - xml은 메시지 이름 마음대로 정할 수 있음

- class 일 경우 .class이름
- id일경우 #id이름
- 아무것도 없을 경우 이름만 검색하면 됌



#### 로또 번호

- 추천 번호랑 지난 로또 번호를 비교해서 중복된 요소 출력

```python
import requests
import random
from bs4 import BeautifulSoup

url = 'https://m.dhlottery.co.kr/common.do?method=main'
response = requests.get(url).text
soup = BeautifulSoup(response, 'html.parser')
document = soup.select('.prizeresult')[0]
numbers = document.select('span')
ns = []
for number in numbers:
  ns.append(int(number.text))

lotto = sorted(random.sample(list(range(1,46)),6))

print(lotto)
print(ns)
#1.-------------------------------------------
#set 함수는 집합으로 나타내기
print(len(list(set(lotto)&set(ns))))
#2.-------------------------------------------
# 지난 주 당첨 숫자 list을 한번씩 순회하면서
# 내가 뽑아놓은 lotto 배열에서
# 몇 개가 맞았는지 count 하기
count = 0

for i in ns:
  for j in lotto:
    if(i==j):
      count+=1
      
# 오래걸림 새로운거
#for j in lotto:
#  if(i==j):
#    count+=1

print(count)
```



#### 네이버웹툰

```python
import requests
import time 
from bs4 import BeautifulSoup as bs

today = time.strftime("%a").lower()
# 1. 네이버 웹툰을 가져올 수 있는 주소 (url)를 파악하고 url 변수에 저장한다
url = 'https://comic.naver.com/webtoon/weekdayList.nhn?week=' + today
# 2. 해당 주소로 요청을 보내 정보를 가져온다.
response = requests.get(url).text
# 3. 받은 정보를 bs를 이용해 검색하기 좋게 만든다.
soup = bs(response, 'html.parser')
# 4. 네이버 웹툰 페이지로 가서, 내가 원하는 정보가 어디에 있는지 검색
# 내가 원하는 정보는 웹툰을 볼 수 있는 링크
# 오늘 자 업데이트 된 웹툰들의 각각 리스트 페이지, 웹툰의 제목 + 해당 웹툰의 썸네일
# 번에서 저장한 문서를 이용해 4번에서 파악하나 위치를 뽑아내는 코드를 작성한다.

toons = []
# class 내에 한번 더 들어가려면 띄어쓰기!
li = soup.select('.img_list li')
for item in li:
  toon = {
    "title" : item.select_one('dt a').text, 
    "url" : 'https://comic.naver.com' + item.select('dt a')[0]["href"],
    "img_url" : item.select('.thumb img')[0]["src"]
  }
  toons.append(toon)
print(toons)
#---------------------------------------------------------------------
name = []
url = []
for i in range(4, 35):
  webtoon = soup.select('.thumb')[i]
  webtoon=webtoon.a
  name.append(webtoon["title"])
  url.append('https://comic.naver.com' + webtoon["href"])
print(name)
print(url)
#webtoon_name = webtoon["title"]
#print(webtoon_name)
#webtoon_url = 'https://comic.naver.com' + webtoon["href"]
#print(webtoon_url)

# 출력한다.
```

- `select('dt a')[0]` 와 `select_one('dt a')` 는 같은 의미!

- 'title'이나 'href'같은 거는 []안에 그냥 넣어서 검색 가능



#### 다음웹툰

```python
import requests
import json
import time
from bs4 import BeautifulSoup as bs

# 1. 내가 원하는 정보를 얻을 수 있는 주소를 url이라고 하는 변수에 담는다.
url = 'http://webtoon.daum.net/data/pc/webtoon/list_serialized/thu'
# 2. 해당 url에 요청을 보내 응답을 받아 저장한다.
response = requests.get(url).text
# 3. 구글신에게 python으로 어떻게 json을 파싱(딕셔너리 형으로 변환)하는지 물어본다
# 4. 파싱한다. (변환한다)
document = json.loads(response)
# 5. 내가 원하는 데이터를 꺼내서 조합한다.
data = document["data"]

toons = []

for toon in data:
  toon = {
    "title" : toon["title"], 
    "url" : 'http://webtoon.daum.net/webtoon/view/' + toon["nickname"], 
    "img_url" : toon["pcThumbnailImage"]["url"]
  }
  toons.append(toon)
print(toons)
```

- Json 크롤링